{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Text_Preprocessing.ipynb","provenance":[{"file_id":"1SQXccQhawAE_Vi1i60b61h-z4o79GdiI","timestamp":1650958121011}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"87E_vZ-kdaCp"},"source":["**Convert text to lowercase.** \n","\n","Example 1. Convert text to lowercase"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DVDbmOwic49j","executionInfo":{"status":"ok","timestamp":1649079850890,"user_tz":-480,"elapsed":6,"user":{"displayName":"Lailatul Qadri Zakaria","userId":"02312798796188474661"}},"outputId":"56ab39c3-bb2d-4c6b-8af2-14fb4275a5e8"},"source":["input_str = \"The 5 biggest countries by population in 2017 are China, India, United States, Indonesia, and Brazil.\"\n","input_str = input_str.lower()\n","print(input_str)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["the 5 biggest countries by population in 2017 are china, india, united states, indonesia, and brazil.\n"]}]},{"cell_type":"markdown","metadata":{"id":"EpxkwsPYdsC4"},"source":["**Remove numbers**\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qASxMaNldxcQ","executionInfo":{"elapsed":17,"status":"ok","timestamp":1649079895420,"user":{"displayName":"Lailatul Qadri Zakaria","userId":"02312798796188474661"},"user_tz":-480},"outputId":"b26bd02d-0e63-4777-a394-914ccbe80b19"},"source":["import re\n","input_str = \"Box A contains 3 red and 5 white balls, while Box B contains 4 red and 2 blue balls.\"\n","result = re.sub(r'\\d+', '', input_str)\n","print(result)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Box A contains  red and  white balls, while Box B contains  red and  blue balls.\n"]}]},{"cell_type":"markdown","metadata":{"id":"BoS0yYmbd83r"},"source":["**Remove punctuation**\n","\n","The following code removes this set of symbols [!”#$%&’()*+,-./:;<=>?@[\\]^_`{|}~]:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q2N0_U9wd-g_","executionInfo":{"elapsed":315,"status":"ok","timestamp":1649079926173,"user":{"displayName":"Lailatul Qadri Zakaria","userId":"02312798796188474661"},"user_tz":-480},"outputId":"416432dd-262e-49ab-9932-9d02680e1cf1"},"source":["\n","\n","import string\n","input_str = \"This &is [an] example? {of} string. with.? punctuation!!!!\" # Sample string\n","\n","result = input_str.translate(str.maketrans('', '', string.punctuation))\n","print(result)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["This is an example of string with punctuation\n"]}]},{"cell_type":"markdown","metadata":{"id":"sX38dk4AeQJ3"},"source":["**Remove whitespaces**\n","\n","To remove leading and ending spaces, you can use the strip() function:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"xsvslWKrePio","executionInfo":{"elapsed":358,"status":"ok","timestamp":1649079954974,"user":{"displayName":"Lailatul Qadri Zakaria","userId":"02312798796188474661"},"user_tz":-480},"outputId":"a28ae138-f967-4589-8d57-e4ab1900ffc5"},"source":["input_str = \" \\t a string example\\t \"\n","input_str = input_str.strip()\n","input_str"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'a string example'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"-IkWk9dCecxP"},"source":["\n","\n","\n","\n","**Remove stop words and tokenization**\n","\n","“Stop words” are the most common words in a language like “the”, “a”, “on”, “is”, “all”. These words do not carry important meaning and are usually removed from texts. It is possible to remove stop words using Natural Language Toolkit (NLTK), a suite of libraries and programs for symbolic and statistical natural language processing."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z3r_iumree3_","executionInfo":{"elapsed":2425,"status":"ok","timestamp":1649080030430,"user":{"displayName":"Lailatul Qadri Zakaria","userId":"02312798796188474661"},"user_tz":-480},"outputId":"3d3bfda6-9ac9-41b6-edc1-861c9f86350c"},"source":["import nltk\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","\n","from nltk.corpus import stopwords \n","input_str = \"Coronavirus disease (COVID-19) is an infectious disease caused by the SARS-CoV-2 virus.\"\n","print(\"\\nInput sentence :\",input_str)\n","stop_words = set(stopwords.words('english'))\n","from nltk.tokenize import word_tokenize\n","tokens = word_tokenize(input_str)\n","print(\"\\nWord tokenization output:\")\n","print(tokens)\n","result = [i for i in tokens if not i in stop_words]\n","for word in result:\n","  print(word)\n","print (result)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","\n","Input sentence : Coronavirus disease (COVID-19) is an infectious disease caused by the SARS-CoV-2 virus.\n","\n","Word tokenization output:\n","['Coronavirus', 'disease', '(', 'COVID-19', ')', 'is', 'an', 'infectious', 'disease', 'caused', 'by', 'the', 'SARS-CoV-2', 'virus', '.']\n","Coronavirus\n","disease\n","(\n","COVID-19\n",")\n","infectious\n","disease\n","caused\n","SARS-CoV-2\n","virus\n",".\n","['Coronavirus', 'disease', '(', 'COVID-19', ')', 'infectious', 'disease', 'caused', 'SARS-CoV-2', 'virus', '.']\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"toX3atg4_bVz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qr9H8jtggRsU"},"source":["**Stemming**\n","\n","Stemming is a process of reducing words to their word stem, base or root form (for example, books — book, looked — look). The main two algorithms are Porter stemming algorithm (removes common morphological and inflexional endings from words [14]) and Lancaster stemming algorithm (a more aggressive stemming algorithm). In the “Stemming” sheet of the table some stemmers are described."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mSrKJTcsgIGV","executionInfo":{"elapsed":314,"status":"ok","timestamp":1649063817895,"user":{"displayName":"Lailatul Qadri Zakaria","userId":"02312798796188474661"},"user_tz":-480},"outputId":"16126b67-c2a4-4354-8319-40b70298a183"},"source":["from nltk.stem import PorterStemmer\n","from nltk.tokenize import word_tokenize\n","stemmer= PorterStemmer()\n","input_str=\"Stemming is a process of reducing words to their word stem, base or root form\"\n","input_str=word_tokenize(input_str)\n","for word in input_str:\n","    print(stemmer.stem(word))\n","print(\">>> sorted list\")\n","sorted_input_str = sorted(input_str)\n","for word_sorted in sorted_input_str:\n","    print(word_sorted)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["stem\n","is\n","a\n","process\n","of\n","reduc\n","word\n","to\n","their\n","word\n","stem\n",",\n","base\n","or\n","root\n","form\n",">>> sorted list\n",",\n","Stemming\n","a\n","base\n","form\n","is\n","of\n","or\n","process\n","reducing\n","root\n","stem\n","their\n","to\n","word\n","words\n"]}]},{"cell_type":"markdown","metadata":{"id":"cGxSibtGgfH0"},"source":["**Lemmatization**\n","\n","The aim of lemmatization, like stemming, is to reduce inflectional forms to a common base form. As opposed to stemming, lemmatization does not simply chop off inflections. Instead it uses lexical knowledge bases to get the correct base forms of words."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tzqp9KhSgjR8","executionInfo":{"elapsed":2952,"status":"ok","timestamp":1649063844035,"user":{"displayName":"Lailatul Qadri Zakaria","userId":"02312798796188474661"},"user_tz":-480},"outputId":"54edae83-cde5-4b74-b854-c47e2ecc137d"},"source":["nltk.download('wordnet')\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import word_tokenize\n","lemmatizer=WordNetLemmatizer()\n","input_str=\"been had done languages cities mice\"\n","input_str=word_tokenize(input_str)\n","for word in input_str:\n","    print(lemmatizer.lemmatize(word))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n","been\n","had\n","done\n","language\n","city\n","mouse\n"]}]},{"cell_type":"markdown","metadata":{"id":"6Jd1Ykvbgx_c"},"source":["**Part of speech tagging (POS)**\n","\n","Part-of-speech tagging aims to assign parts of speech to each word of a given text (such as nouns, verbs, adjectives, and others) based on its definition and its context. There are many tools containing POS taggers including NLTK, spaCy, TextBlob, Pattern, Stanford CoreNLP, Memory-Based Shallow Parser (MBSP), Apache OpenNLP, Apache Lucene, General Architecture for Text Engineering (GATE), FreeLing, Illinois Part of Speech Tagger, and DKPro Core."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":105},"id":"9PEbOvZeg1vV","executionInfo":{"elapsed":1049,"status":"ok","timestamp":1581997715160,"user":{"displayName":"Lailatul Qadri Zakaria","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBc1gDjlk0wcvEKaDIR2W9NzaUSrE63sJSq94n81g=s64","userId":"02312798796188474661"},"user_tz":-480},"outputId":"e428168f-cfb2-4d3c-8628-1a61746725a4"},"source":["nltk.download('averaged_perceptron_tagger')\n","input_str=\"Parts of speech examples: an article, to write, interesting, easily, and, of\"\n","from textblob import TextBlob\n","result = TextBlob(input_str)\n","print(result.tags)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[('Parts', 'NNS'), ('of', 'IN'), ('speech', 'NN'), ('examples', 'NNS'), ('an', 'DT'), ('article', 'NN'), ('to', 'TO'), ('write', 'VB'), ('interesting', 'VBG'), ('easily', 'RB'), ('and', 'CC'), ('of', 'IN')]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"id":"lGrt7C3UihEJ","executionInfo":{"elapsed":764,"status":"ok","timestamp":1581945604114,"user":{"displayName":"Lailatul Qadri Zakaria","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBc1gDjlk0wcvEKaDIR2W9NzaUSrE63sJSq94n81g=s64","userId":"02312798796188474661"},"user_tz":-480},"outputId":"d56354d7-9549-41e9-feed-ad721f2cb2e1"},"source":["input_str=\"A black television and a white stove were bought for the new apartment of John.\"\n","from textblob import TextBlob\n","result = TextBlob(input_str)\n","print(result.tags)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[('A', 'DT'), ('black', 'JJ'), ('television', 'NN'), ('and', 'CC'), ('a', 'DT'), ('white', 'JJ'), ('stove', 'NN'), ('were', 'VBD'), ('bought', 'VBN'), ('for', 'IN'), ('the', 'DT'), ('new', 'JJ'), ('apartment', 'NN'), ('of', 'IN'), ('John', 'NNP')]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":187},"id":"DWKC604Gisxw","executionInfo":{"elapsed":755,"status":"ok","timestamp":1581945629177,"user":{"displayName":"Lailatul Qadri Zakaria","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBc1gDjlk0wcvEKaDIR2W9NzaUSrE63sJSq94n81g=s64","userId":"02312798796188474661"},"user_tz":-480},"outputId":"82941234-90c5-40a9-f079-5c7c4a14fa87"},"source":["reg_exp = \"NP: {<DT>?<JJ>*<NN>}\"\n","rp = nltk.RegexpParser(reg_exp)\n","result = rp.parse(result.tags)\n","print(result)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(S\n","  (NP A/DT black/JJ television/NN)\n","  and/CC\n","  (NP a/DT white/JJ stove/NN)\n","  were/VBD\n","  bought/VBN\n","  for/IN\n","  (NP the/DT new/JJ apartment/NN)\n","  of/IN\n","  John/NNP)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"m86yQxQwjhWG"},"source":["**Named-entity recognition using NLTK**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":340},"id":"OaQ0duZOi0Lv","executionInfo":{"elapsed":1939,"status":"ok","timestamp":1581997792130,"user":{"displayName":"Lailatul Qadri Zakaria","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBc1gDjlk0wcvEKaDIR2W9NzaUSrE63sJSq94n81g=s64","userId":"02312798796188474661"},"user_tz":-480},"outputId":"007b6d2d-42c5-496a-9e4f-551e8698ad90"},"source":["nltk.download('maxent_ne_chunker')\n","nltk.download('words')\n","from nltk import word_tokenize, pos_tag, ne_chunk\n","input_str = \"Bill works for Apple so he went to Boston for a conference.\"\n","print(ne_chunk(pos_tag(word_tokenize(input_str))))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package maxent_ne_chunker to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n","[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/words.zip.\n","(S\n","  (PERSON Bill/NNP)\n","  works/VBZ\n","  for/IN\n","  Apple/NNP\n","  so/IN\n","  he/PRP\n","  went/VBD\n","  to/TO\n","  (GPE Boston/NNP)\n","  for/IN\n","  a/DT\n","  conference/NN\n","  ./.)\n"],"name":"stdout"}]}]}