{"cells":[{"cell_type":"markdown","metadata":{"id":"9lsrugYtzh-u"},"source":["# Text Normalization\n"]},{"cell_type":"markdown","metadata":{"id":"LMmdt10szh-z"},"source":["## Overview"]},{"cell_type":"markdown","metadata":{"id":"PtPRuPTIzh-0"},"source":["The objective of text normalization is to clean up the text by removing unnecessary and irrelevant components.\n","- reducing the number of unique tokens presen in the text\n","- removing the variations in a text. \n","- removing redundant information\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EPrAU_6wzh-0"},"outputs":[],"source":["import spacy\n","import unicodedata\n","import re\n","from nltk.corpus import wordnet\n","import collections\n","from nltk.tokenize.toktok import ToktokTokenizer\n"]},{"cell_type":"markdown","metadata":{"id":"X7qKJ927zh-6"},"source":["## Stemming"]},{"cell_type":"markdown","metadata":{"id":"AnQ-UaTCzh-7"},"source":["- Stemming is the process where we standardize word forms into their base stem irrespective of their inflections.\n","- The `nltk` provides several popular stemmers for English:\n","    - `nltk.stem.PorterStemmer`\n","    - `nltk.stem.LancasterStemmer`\n","    - `nltk.stem.RegexpStemmer`\n","    - `nltk.stem.SnowballStemmer`"]},{"cell_type":"markdown","metadata":{"id":"xQZhPdGfzh-8"},"source":["- We can compare the results of different stemmers."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O-s-_rl5zh-8"},"outputs":[],"source":["import nltk\n","from nltk.stem import PorterStemmer, LancasterStemmer, RegexpStemmer, SnowballStemmer\n","\n","words = ['jumping', 'jumps', 'jumped', 'jumpy']\n","ps = PorterStemmer()\n","ls = LancasterStemmer()\n","ss = SnowballStemmer('english')\n","\n","rs = RegexpStemmer('ing$|s$|ed$|y$', min=4) # set the minimum of the string to stem\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LRjBMoyFzh-8","executionInfo":{"status":"ok","timestamp":1650858352212,"user_tz":-480,"elapsed":17,"user":{"displayName":"CHONG WEI YI","userId":"01816057422713521367"}},"outputId":"09eeeb52-ff30-4693-cb94-fc0f5e1bb9de","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['jump', 'jump', 'jump', 'jumpi']"]},"metadata":{},"execution_count":3}],"source":["[ps.stem(w) for w in words]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eEinxn3bzh-9","executionInfo":{"status":"ok","timestamp":1650858352212,"user_tz":-480,"elapsed":14,"user":{"displayName":"CHONG WEI YI","userId":"01816057422713521367"}},"outputId":"4797d902-5af7-4599-c1db-89512cf7c6f6","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['jump', 'jump', 'jump', 'jumpy']"]},"metadata":{},"execution_count":4}],"source":["[ls.stem(w) for w in words]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2eWnnbrazh-9","executionInfo":{"status":"ok","timestamp":1650858352213,"user_tz":-480,"elapsed":12,"user":{"displayName":"CHONG WEI YI","userId":"01816057422713521367"}},"outputId":"e04476ea-5458-44f4-fb7e-3cc9a9271443","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['jump', 'jump', 'jump', 'jumpi']"]},"metadata":{},"execution_count":5}],"source":["[ss.stem(w) for w in words]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3qIEjAhCzh--","executionInfo":{"status":"ok","timestamp":1650858352213,"user_tz":-480,"elapsed":10,"user":{"displayName":"CHONG WEI YI","userId":"01816057422713521367"}},"outputId":"2f326dde-d1c2-41c4-d8d3-bc93a30e6ab3","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['jump', 'jump', 'jump', 'jump']"]},"metadata":{},"execution_count":6}],"source":["[rs.stem(w) for w in words]"]},{"cell_type":"markdown","metadata":{"id":"i1YIibn1zh--"},"source":["## Lemmatization\n"]},{"cell_type":"markdown","metadata":{"id":"Ip-ZFVLjzh--"},"source":["- Lemmatization is similar to stemmatization.\n","- It is a process where we remove word affixes to get the **root word** but not the **root stem**.\n","- These root words, i.e., lemmas, are lexicographically correct words and always present in the dictionary."]},{"cell_type":"markdown","metadata":{"id":"aC7mcWFYzh--"},"source":["```{admonition} Question\n",":class: attention\n","In terms of Lemmatization and Stemmatization, which one requires more computational cost? That is, which processing might be slower?\n","```"]},{"cell_type":"markdown","metadata":{"id":"TvyKoHTszh-_"},"source":["- Two frequently-used lemmatizers\n","    - `nltk.stem.WordNetLemmatizer`\n","    - `spacy`"]},{"cell_type":"markdown","metadata":{"id":"Hf63aPklzh-_"},"source":["### WordNet Lemmatizer"]},{"cell_type":"markdown","metadata":{"id":"ZbHPdNv0zh-_"},"source":["- WordNetLemmatizer utilizes the dictionary of WordNet.\n","- It requires the **parts of speech** of the word for lemmatization.\n","- I think right now only nouns, verbs and adjectives are important in `WordNetLemmatizer`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YUd03F6Jzh_A"},"outputs":[],"source":["from nltk.stem import WordNetLemmatizer\n","wnl = WordNetLemmatizer()"]},{"cell_type":"code","source":["nltk.download('wordnet')"],"metadata":{"id":"SU20orwm0QLo","executionInfo":{"status":"ok","timestamp":1650858352770,"user_tz":-480,"elapsed":564,"user":{"displayName":"CHONG WEI YI","userId":"01816057422713521367"}},"outputId":"c55adf9c-4bc0-4d4b-91b1-a230a2b4a4f6","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Th-dcVa6zh_A","executionInfo":{"status":"ok","timestamp":1650858355921,"user_tz":-480,"elapsed":3154,"user":{"displayName":"CHONG WEI YI","userId":"01816057422713521367"}},"outputId":"252367d0-ae26-4b92-cb16-1e6c08692455","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["car\n","men\n"]}],"source":["# nouns\n","print(wnl.lemmatize('cars','n')) # noun\n","print(wnl.lemmatize('men', 'n'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lZGgH-70zh_B","executionInfo":{"status":"ok","timestamp":1650858355922,"user_tz":-480,"elapsed":16,"user":{"displayName":"CHONG WEI YI","userId":"01816057422713521367"}},"outputId":"097b645f-b40a-400a-e669-d494f479c7d8","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["run\n","eat\n"]}],"source":["# verbs\n","print(wnl.lemmatize('running','v')) # verb\n","print(wnl.lemmatize('ate', 'v'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GrS9EVO8zh_B","executionInfo":{"status":"ok","timestamp":1650858355922,"user_tz":-480,"elapsed":12,"user":{"displayName":"CHONG WEI YI","userId":"01816057422713521367"}},"outputId":"fc7392b1-e829-4b5d-9bb5-ab0cc57a6597","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["sad\n","fancy\n","jumpy\n"]}],"source":["# adj\n","print(wnl.lemmatize('saddest','a')) # adjectives\n","print(wnl.lemmatize('fancier','a'))\n","print(wnl.lemmatize('jumpy','a'))"]},{"cell_type":"markdown","metadata":{"id":"4lOAZpjGzh_D"},"source":["## Contractions"]},{"cell_type":"markdown","metadata":{"id":"-4Z5ZDLwzh_D"},"source":["- For the English data, contractions are problematic sometimes. \n","- These may get even more complicated when different tokenizers deal with contractions differently.\n","- A good way is to expand all contractions into their original independent word forms."]},{"cell_type":"code","source":["!pip install contractions"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tGAI6i2F2ETj","executionInfo":{"status":"ok","timestamp":1650858363297,"user_tz":-480,"elapsed":7384,"user":{"displayName":"CHONG WEI YI","userId":"01816057422713521367"}},"outputId":"e1570325-0ad1-4814-abed-74d39611cfb8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting contractions\n","  Downloading contractions-0.1.72-py2.py3-none-any.whl (8.3 kB)\n","Collecting textsearch>=0.0.21\n","  Downloading textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n","Collecting anyascii\n","  Downloading anyascii-0.3.1-py3-none-any.whl (287 kB)\n","\u001b[K     |████████████████████████████████| 287 kB 8.4 MB/s \n","\u001b[?25hCollecting pyahocorasick\n","  Downloading pyahocorasick-1.4.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n","\u001b[K     |████████████████████████████████| 106 kB 44.5 MB/s \n","\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n","Successfully installed anyascii-0.3.1 contractions-0.1.72 pyahocorasick-1.4.4 textsearch-0.0.21\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y44MzKiczh_E","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650858363298,"user_tz":-480,"elapsed":19,"user":{"displayName":"CHONG WEI YI","userId":"01816057422713521367"}},"outputId":"5d234812-e494-4604-c64d-e32d89b79b46"},"outputs":[{"output_type":"stream","name":"stdout","text":["Original text: I'll be there within 5 min. Shouldn't you be there too?\n","          I'd love to see u there my dear. It's awesome to meet new friends.\n","          We've been waiting for this day for so long.\n","Expanded_text: I will be there within 5 min. Should not you be there too? I would love to see you there my dear. It is awesome to meet new friends. We have been waiting for this day for so long.\n"]}],"source":["# import library\n","import contractions\n","# contracted text\n","text = '''I'll be there within 5 min. Shouldn't you be there too?\n","          I'd love to see u there my dear. It's awesome to meet new friends.\n","          We've been waiting for this day for so long.'''\n"," \n","# creating an empty list\n","expanded_words = []   \n","for word in text.split():\n","  # using contractions.fix to expand the shortened words\n","  expanded_words.append(contractions.fix(word))  \n","   \n","expanded_text = ' '.join(expanded_words)\n","print('Original text: ' + text)\n","print('Expanded_text: ' + expanded_text)"]},{"cell_type":"code","source":["text = '''She'd like to know how I'd done that!\n","          She's going to the park and I don't think I'll be home for dinner.\n","          Theyre going to the zoo and she'll be home for dinner.'''\n"," \n","contractions.fix(text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"id":"tDFS4Bj62zfa","executionInfo":{"status":"ok","timestamp":1650858363299,"user_tz":-480,"elapsed":16,"user":{"displayName":"CHONG WEI YI","userId":"01816057422713521367"}},"outputId":"b4e0efdc-2ff1-4e29-b2f0-7fd6602391a6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'She would like to know how I would done that!\\n          She is going to the park and I do not think I will be home for dinner.\\n          They Are going to the zoo and she will be home for dinner.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"v6-Jp7m1zh_F"},"source":["## Accented Characters (Non-ASCII)\n","\n","- The `unicodedata` module handles unicode characters very efficiently. Please check [unicodedata dcoumentation](https://docs.python.org/3/library/unicodedata.html) for more details.\n","- When dealing with the English data, we may often encounter foreign characters in texts that are not part of the ASCII character set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-WNgwxnlzh_F","outputId":"37c4dd12-4b2a-4895-a168-9095213dafac","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1650858363300,"user_tz":-480,"elapsed":15,"user":{"displayName":"CHONG WEI YI","userId":"01816057422713521367"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Some Accented text'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":15}],"source":["import unicodedata\n","\n","def remove_accented_chars(text):\n","#     ```\n","#     (NFKD) will apply the compatibility decomposition, i.e. \n","#     replace all compatibility characters with their equivalents. \n","#     ```\n","    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n","    return text\n","\n","\n","remove_accented_chars('Sómě Áccěntěd těxt')\n","\n","# print(unicodedata.normalize('NFKD', 'Sómě Áccěntěd těxt'))\n","# print(unicodedata.normalize('NFKD', 'Sómě Áccěntěd těxt').encode('ascii','ignore'))\n","# print(unicodedata.normalize('NFKD', 'Sómě Áccěntěd těxt').encode('ascii','ignore').decode('utf-8', 'ignore'))"]},{"cell_type":"markdown","metadata":{"id":"zSafQ-P9zh_G"},"source":[":::{note}\n","- `str.encode()` returns an encoded version of the string as a bytes object using the specified encoding.\n","- `byes.decode()` returns a string decoded from the given bytes using the specified encoding.\n",":::"]},{"cell_type":"markdown","source":["## Converting emojis to text\n","\n"],"metadata":{"id":"FQpHhl8_NqbD"}},{"cell_type":"markdown","source":["import demoji\n"," \n","demoji.download_codes()"],"metadata":{"id":"3pY-1isTV6VB"}},{"cell_type":"code","source":["!pip install demoji"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IFE0D177V8Y1","executionInfo":{"status":"ok","timestamp":1650858367444,"user_tz":-480,"elapsed":4157,"user":{"displayName":"CHONG WEI YI","userId":"01816057422713521367"}},"outputId":"930e6436-bb78-4fb0-ed0a-a6fd546f214a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting demoji\n","  Downloading demoji-1.1.0-py3-none-any.whl (42 kB)\n","\u001b[?25l\r\u001b[K     |███████▋                        | 10 kB 22.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 20 kB 25.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 30 kB 29.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 40 kB 23.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 42 kB 952 kB/s \n","\u001b[?25hInstalling collected packages: demoji\n","Successfully installed demoji-1.1.0\n"]}]},{"cell_type":"code","source":["import demoji\n"," \n","demoji.download_codes()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tG3I_-VJV7K8","executionInfo":{"status":"ok","timestamp":1650858367445,"user_tz":-480,"elapsed":18,"user":{"displayName":"CHONG WEI YI","userId":"01816057422713521367"}},"outputId":"937a4edf-3367-4776-9b37-632ecb42ff3a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: FutureWarning: The demoji.download_codes attribute is deprecated and will be removed from demoji in a future version. It is an unused attribute as emoji codes are now distributed directly with the demoji package.\n","  This is separate from the ipykernel package so we can avoid doing imports until\n"]}]},{"cell_type":"code","source":["text = \"i am happy 😁\"\n","demoji.findall(text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4iHWlatlWA-r","executionInfo":{"status":"ok","timestamp":1650858367447,"user_tz":-480,"elapsed":17,"user":{"displayName":"CHONG WEI YI","userId":"01816057422713521367"}},"outputId":"0dea61d4-14ce-421e-9e8f-7e347ab22e51"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'😁': 'beaming face with smiling eyes'}"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","source":["## Slang word"],"metadata":{"id":"HIrBoj8oXJTl"}},{"cell_type":"code","source":["\n","input = 'ok, tq dr'\n","output = input.replace('ok','okey')\n","output = output.replace('tq','thank you')\n","output = output.replace('dr','Doctor')\n","\n","\n","print(output)\n","# Kita tidak bisa melakukan ini"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ed0YUMa3XK_7","executionInfo":{"status":"ok","timestamp":1650858367449,"user_tz":-480,"elapsed":17,"user":{"displayName":"CHONG WEI YI","userId":"01816057422713521367"}},"outputId":"805353cb-b8b4-489b-b71e-3a0fe73c3eac"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["okey, thank you Doctor\n"]}]},{"cell_type":"markdown","source":["Or you also can you a dictionary"],"metadata":{"id":"Z-NKmI0hYdD8"}},{"cell_type":"code","source":["dictionary_slang = {\n","    'ok':'okey', \n","    'tq':'thank you', \n","    'dr':'Doctor',\n","    }\n","    \n","dictionary_slang['tq']\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"i31d4gTqYcb0","executionInfo":{"status":"ok","timestamp":1650858367450,"user_tz":-480,"elapsed":15,"user":{"displayName":"CHONG WEI YI","userId":"01816057422713521367"}},"outputId":"54734e47-81f6-47af-82d1-238b24113a74"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'thank you'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","source":["## Translation mix language"],"metadata":{"id":"NJz2pN4siP00"}},{"cell_type":"code","source":["!pip install googletrans"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WjIAl3ZVg9Cl","executionInfo":{"status":"ok","timestamp":1650858374483,"user_tz":-480,"elapsed":7047,"user":{"displayName":"CHONG WEI YI","userId":"01816057422713521367"}},"outputId":"1d20a5a4-175e-4d6c-bb19-2c60ae3f98c1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting googletrans\n","  Downloading googletrans-3.0.0.tar.gz (17 kB)\n","Collecting httpx==0.13.3\n","  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n","\u001b[K     |████████████████████████████████| 55 kB 2.6 MB/s \n","\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans) (2021.10.8)\n","Requirement already satisfied: idna==2.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans) (2.10)\n","Collecting rfc3986<2,>=1.3\n","  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n","Collecting sniffio\n","  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n","Collecting httpcore==0.9.*\n","  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n","\u001b[K     |████████████████████████████████| 42 kB 1.0 MB/s \n","\u001b[?25hRequirement already satisfied: chardet==3.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans) (3.0.4)\n","Collecting hstspreload\n","  Downloading hstspreload-2021.12.1-py3-none-any.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 18.2 MB/s \n","\u001b[?25hCollecting h2==3.*\n","  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n","\u001b[K     |████████████████████████████████| 65 kB 3.1 MB/s \n","\u001b[?25hCollecting h11<0.10,>=0.8\n","  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n","\u001b[K     |████████████████████████████████| 53 kB 1.9 MB/s \n","\u001b[?25hCollecting hpack<4,>=3.0\n","  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n","Collecting hyperframe<6,>=5.2.0\n","  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n","Building wheels for collected packages: googletrans\n","  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for googletrans: filename=googletrans-3.0.0-py3-none-any.whl size=15735 sha256=c920862345af3dd8ed2b5b93977c12e7c7ea2cb640706d6336695dd15e2bbbb3\n","  Stored in directory: /root/.cache/pip/wheels/20/da/eb/a54579056f265eede0417df537dd56d3df5b9eb2b25df0003d\n","Successfully built googletrans\n","Installing collected packages: hyperframe, hpack, sniffio, h2, h11, rfc3986, httpcore, hstspreload, httpx, googletrans\n","Successfully installed googletrans-3.0.0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2021.12.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 rfc3986-1.5.0 sniffio-1.2.0\n"]}]},{"cell_type":"code","source":["import googletrans"],"metadata":{"id":"wfYb2pmlhbHL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from googletrans import Translator"],"metadata":{"id":"LAq-7pNRhFQB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(googletrans.LANGUAGES)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tEPztJMvhXCa","executionInfo":{"status":"ok","timestamp":1650858375268,"user_tz":-480,"elapsed":22,"user":{"displayName":"CHONG WEI YI","userId":"01816057422713521367"}},"outputId":"a960a38f-0dec-4c3a-a9d7-e7985190d94b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'af': 'afrikaans', 'sq': 'albanian', 'am': 'amharic', 'ar': 'arabic', 'hy': 'armenian', 'az': 'azerbaijani', 'eu': 'basque', 'be': 'belarusian', 'bn': 'bengali', 'bs': 'bosnian', 'bg': 'bulgarian', 'ca': 'catalan', 'ceb': 'cebuano', 'ny': 'chichewa', 'zh-cn': 'chinese (simplified)', 'zh-tw': 'chinese (traditional)', 'co': 'corsican', 'hr': 'croatian', 'cs': 'czech', 'da': 'danish', 'nl': 'dutch', 'en': 'english', 'eo': 'esperanto', 'et': 'estonian', 'tl': 'filipino', 'fi': 'finnish', 'fr': 'french', 'fy': 'frisian', 'gl': 'galician', 'ka': 'georgian', 'de': 'german', 'el': 'greek', 'gu': 'gujarati', 'ht': 'haitian creole', 'ha': 'hausa', 'haw': 'hawaiian', 'iw': 'hebrew', 'he': 'hebrew', 'hi': 'hindi', 'hmn': 'hmong', 'hu': 'hungarian', 'is': 'icelandic', 'ig': 'igbo', 'id': 'indonesian', 'ga': 'irish', 'it': 'italian', 'ja': 'japanese', 'jw': 'javanese', 'kn': 'kannada', 'kk': 'kazakh', 'km': 'khmer', 'ko': 'korean', 'ku': 'kurdish (kurmanji)', 'ky': 'kyrgyz', 'lo': 'lao', 'la': 'latin', 'lv': 'latvian', 'lt': 'lithuanian', 'lb': 'luxembourgish', 'mk': 'macedonian', 'mg': 'malagasy', 'ms': 'malay', 'ml': 'malayalam', 'mt': 'maltese', 'mi': 'maori', 'mr': 'marathi', 'mn': 'mongolian', 'my': 'myanmar (burmese)', 'ne': 'nepali', 'no': 'norwegian', 'or': 'odia', 'ps': 'pashto', 'fa': 'persian', 'pl': 'polish', 'pt': 'portuguese', 'pa': 'punjabi', 'ro': 'romanian', 'ru': 'russian', 'sm': 'samoan', 'gd': 'scots gaelic', 'sr': 'serbian', 'st': 'sesotho', 'sn': 'shona', 'sd': 'sindhi', 'si': 'sinhala', 'sk': 'slovak', 'sl': 'slovenian', 'so': 'somali', 'es': 'spanish', 'su': 'sundanese', 'sw': 'swahili', 'sv': 'swedish', 'tg': 'tajik', 'ta': 'tamil', 'te': 'telugu', 'th': 'thai', 'tr': 'turkish', 'uk': 'ukrainian', 'ur': 'urdu', 'ug': 'uyghur', 'uz': 'uzbek', 'vi': 'vietnamese', 'cy': 'welsh', 'xh': 'xhosa', 'yi': 'yiddish', 'yo': 'yoruba', 'zu': 'zulu'}\n"]}]},{"cell_type":"code","source":["translator = Translator()"],"metadata":{"id":"Fy47dYs8hgkQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result = translator.translate(\"selamat\", src=\"ms\", dest=\"en\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":311},"id":"EVSx7be5hk-Y","executionInfo":{"status":"error","timestamp":1650858376015,"user_tz":-480,"elapsed":760,"user":{"displayName":"CHONG WEI YI","userId":"01816057422713521367"}},"outputId":"61dfa72b-6937-4d8f-d645-973ea76ede10"},"execution_count":null,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-26-35f528b83c68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"selamat\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ms\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"en\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/googletrans/client.py\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(self, text, dest, src, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0morigin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_translate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;31m# this code will be updated when the format is changed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/googletrans/client.py\u001b[0m in \u001b[0;36m_translate\u001b[0;34m(self, text, dest, src, override)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_translate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_acquirer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         params = utils.build_params(query=text, src=src, dest=dest,\n\u001b[1;32m     80\u001b[0m                                     token=token, override=override)\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/googletrans/gtoken.py\u001b[0m in \u001b[0;36mdo\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m         \u001b[0mtk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/googletrans/gtoken.py\u001b[0m in \u001b[0;36m_update\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m# this will be the same as python code after stripping out a reserved word 'var'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRE_TKK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'var '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;31m# unescape special ascii characters such like a \\x3d(=)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unicode-escape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'group'"]}]}],"metadata":{"celltoolbar":"Slideshow","kernelspec":{"display_name":"python-notes","language":"python","name":"python-notes"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":false,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"colab":{"name":"Text-normalization.ipynb","provenance":[{"file_id":"1PgVs2MqeuTNlYL2iZvMcl8KFIpCklbGQ","timestamp":1650858171595},{"file_id":"https://github.com/alvinntnu/NTNU_ENC2045_LECTURES/blob/main/nlp/text-normalization-eng.ipynb","timestamp":1650807923816}],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}